{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# one off install\n",
    "#%pip install torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise - display a 3 dimensional (random) tensor: five 3*2 matrices \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import functions used in this Case Study\n",
    "import PyTorch_Functions as ptf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Prep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise - \n",
    "\n",
    "# a) download data from zip file: https://download.pytorch.org/tutorial/data.zip\n",
    "# b) extract it\n",
    "# c) rename folder \"PyTorch-NLPdata\"\n",
    "# d) put it in Jupyter working directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# aside Jupyter working directory\n",
    "# pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import unicode_literals, print_function, division\n",
    "from io import open\n",
    "import os\n",
    "\n",
    "print(ptf.findFiles('PyTorch-NLPdata/data/names/*.txt'))\n",
    "\n",
    "# Build the category_lines dictionary, a list of names per language\n",
    "category_lines = {}\n",
    "all_categories = []\n",
    "\n",
    "for filename in ptf.findFiles('PyTorch-NLPdata/data/names/*.txt'):\n",
    "    category = os.path.splitext(os.path.basename(filename))[0]\n",
    "    all_categories.append(category)\n",
    "    lines = ptf.readLines(filename)\n",
    "    category_lines[category] = lines\n",
    "\n",
    "n_categories = len(all_categories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "category_lines.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "category_lines.values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise display last 5 Portuguese names\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.manual_seed(0) ADD THIS IF WANT TO REPRODUCE RESULTS!\n",
    "\n",
    "n_hidden = 128\n",
    "\n",
    "rnn = ptf.RNN(ptf.n_letters, n_hidden, n_categories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run a network step (one forward pass)\n",
    "input = ptf.letterToTensor('A')\n",
    "hidden = torch.zeros(1, n_hidden)\n",
    "\n",
    "output, next_hidden = rnn(input, hidden)\n",
    "print(output)\n",
    "\n",
    "print(ptf.categoryFromOutput(all_categories, output)) # most likely language for the letter A (from ALbert)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "output shows a tensor showing the (log) probability/likelihood of the letter A\n",
    "being in each of the 18 dictionaries. The tuple at the end refers to the most likely language\n",
    "to which it belongs (highest log value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NB above gives different results if you recreate the RNN network above without \"seeding\" - add torch.manual_seed(0) to recreate results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model Prep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get a training example (a name and its language):\n",
    "\n",
    "# calls the randomTrainingExample function ten times\n",
    "for i in range(10):\n",
    "    category, line, category_tensor, line_tensor = ptf.randomTrainingExample(all_categories, category_lines)\n",
    "    print('category =', category, '/ line =', line) # category is language\n",
    "    # line is name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training the Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training iterations below will:\n",
    "\n",
    "- Create input and target tensors\n",
    "- Create a zeroed initial hidden state\n",
    "- Read each letter in and\n",
    "- Keep hidden state for next letter\n",
    "- Compare final output to target\n",
    "- Back-propagate\n",
    "- Return the output and loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.05 # If you set this too high, it might explode. \n",
    "# If too low, it might not learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train with a bunch of examples\n",
    "import time\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "n_iters = 5000 # keep smaller for training purposes NB use 5000 - quite fast\n",
    "# in practise could use e.g. 100000 iterations for training\n",
    "print_every = 500 # divide n_iters by 10\n",
    "plot_every = 100 # divide n_iters by 50\n",
    "\n",
    "# Keep track of losses for plotting\n",
    "current_loss = 0\n",
    "all_losses = []\n",
    "\n",
    "for iter in range(1, n_iters + 1):\n",
    "    category, line, category_tensor, line_tensor = ptf.randomTrainingExample(all_categories, category_lines)\n",
    "    output, loss = ptf.train(category_tensor, \n",
    "                             line_tensor, \n",
    "                             rnn, \n",
    "                             learning_rate)\n",
    "    current_loss += loss\n",
    "\n",
    "    # Print iter number, loss, name and guess\n",
    "    if iter % print_every == 0:\n",
    "        guess, guess_i = ptf.categoryFromOutput(all_categories, output)\n",
    "        correct = '✓' if guess == category else '✗ (%s)' % category\n",
    "        print('%d %d%% (%s) %.4f %s / %s %s' % (iter, iter / n_iters * 100, ptf.timeSince(start), loss, line, guess, correct))\n",
    "\n",
    "    # Add current loss avg to list of losses\n",
    "    if iter % plot_every == 0:\n",
    "        all_losses.append(current_loss / plot_every)\n",
    "        current_loss = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plotting the Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(all_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(all_losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluating the Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate TEST surnames and produce a confusion matrix\n",
    "# comparing actual language and predicted language\n",
    "\n",
    "# Keep track of correct guesses in a confusion matrix\n",
    "confusion = torch.zeros(n_categories, n_categories)\n",
    "n_confusion = 1000\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.ticker as ticker\n",
    "\n",
    "column_names = [\"record\", \"name\", \"actual\", \"predicted\",\"predicted-lng\"] # information we want to isolate/export from\n",
    "\n",
    "mydf = pd.DataFrame(columns = column_names) # define an empty dataframe we will populate in the for loop\n",
    "likelihood_df = pd.DataFrame(columns = [\"likelihood\"])\n",
    "allProbs =pd.DataFrame(columns = range(0,18)) # easier way to extract language probabilities\n",
    "\n",
    "# Go through a bunch of examples and record which are correctly guessed\n",
    "for i in range(n_confusion):\n",
    "    category, line, category_tensor, line_tensor = ptf.randomTrainingExample(all_categories, category_lines)\n",
    "    output = ptf.evaluate(line_tensor, rnn)\n",
    "    guess, guess_i = ptf.categoryFromOutput(all_categories, output)\n",
    "    category_i = all_categories.index(category)\n",
    "    confusion[category_i][guess_i] += 1\n",
    "    \n",
    "    # extract actual and predicted language for all 1000 samples above     mydf.loc[i, \"record\"] = i\n",
    "    mydf.loc[i, \"name\"] = line\n",
    "    mydf.loc[i, \"actual\"] = category_i # actual language index\n",
    "    mydf.loc[i, \"predicted\"] = guess_i # predicted language index\n",
    "    mydf.loc[i, \"predicted-lng\"] = guess # predicted language \n",
    "    likelihood_df.loc[i,\"likelihood\"] = output.detach().numpy()  # transform the likelihhod tensor into\n",
    "    # easier way to extract language probabilities\n",
    "    for j in range(18):\n",
    "        allProbs.loc[i,j] = output.detach().numpy()[0][j]\n",
    "\n",
    "# Normalize by dividing every row by its sum\n",
    "for i in range(n_categories):\n",
    "    confusion[i] = confusion[i] / confusion[i].sum()\n",
    "\n",
    "# Set up plot\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "cax = ax.matshow(confusion.numpy())\n",
    "fig.colorbar(cax)\n",
    "\n",
    "# Set up axes\n",
    "ax.set_xticklabels([''] + all_categories, rotation=90)\n",
    "ax.set_yticklabels([''] + all_categories)\n",
    "\n",
    "# Force label at every tick\n",
    "ax.xaxis.set_major_locator(ticker.MultipleLocator(1))\n",
    "ax.yaxis.set_major_locator(ticker.MultipleLocator(1))\n",
    "\n",
    "# sphinx_gallery_thumbnail_number = 2\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NB confusion matrix is PROBABILISTIC - it considers the probability a\n",
    "name is any of the 18 languages and adds these up over all the iterations -\n",
    "so even if the predicted language is not the highest probability, the\n",
    "relative probability is still high enough (e.g. top 3 / top 5) to show the overall picture above of a good model over many interations. This is what we confirm in the export below i.e. even for incorrect classifications, the probability of the actual class (language) is still one of the highest \n",
    "for each name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Exercise - \n",
    "# a) extract actual and predicted language for all 10000 samples above and \n",
    "# b) export to a csv\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

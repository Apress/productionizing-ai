{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "02a38e3c",
   "metadata": {},
   "source": [
    "#### Word2Vec"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64704340",
   "metadata": {},
   "source": [
    "GOAL: Create (or import) unstructured text and use Word2Vec to convert into vectors. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be8d33fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# libraries\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "import json\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "import string\n",
    "\n",
    "from textblob import Word\n",
    "\n",
    "from gensim.models import Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b4fd210",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import data\n",
    "\n",
    "json_file ='intents.json'\n",
    "with open('intents.json','r') as f:\n",
    "    data = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7910e5b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd871075",
   "metadata": {},
   "source": [
    "series of patterns and responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89c5f2d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(data) # check number of intents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "053782c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert to dataframe\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "df['patterns'] = df['patterns'].apply(', '.join) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ce83c6f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df # quick check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5630973e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# wrangling - cleanup the text\n",
    "\n",
    "# identify stopwords\n",
    "stop = stopwords.words('english') \n",
    "\n",
    "# convert all words to lower case\n",
    "df['patterns'] = df['patterns'].apply(lambda x:' '.join(x.lower() for x in x.split()))\n",
    "\n",
    "#filter out string punctuation\n",
    "df['patterns'] = df['patterns'].apply(lambda x: ' '.join(x for x in x.split() if x not in string.punctuation))\n",
    "\n",
    "#remove numbers or \".\" using a regular expression                                        \n",
    "df['patterns'] = df['patterns'].str.replace('[^\\w\\s]','')\n",
    "                                        \n",
    "# remove digits                                      \n",
    "df['patterns']= df['patterns'].apply(lambda x: ' '.join(x for x in x.split() if  not x.isdigit()))\n",
    "\n",
    "# # now remove stop words                                        \n",
    "df['patterns'] = df['patterns'].apply(lambda x:' '.join(x for x in x.split() if not x in stop))\n",
    "\n",
    "# # lemmatization used to remove different forms of the same word\n",
    "df['patterns'] = df['patterns'].apply(lambda x: \" \".join([Word(word).lemmatize() for word in x.split()]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18cb650e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#check\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b9a02b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#check\n",
    "df['patterns']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c278c751",
   "metadata": {},
   "outputs": [],
   "source": [
    "# build a model using Word2Vec\n",
    "\n",
    "bigger_list=[] # create an empty list\n",
    "\n",
    "# loop thru each entry in df['patterns'] and split it into words\n",
    "# append these to the \"bigger_list\"\n",
    "for i in df['patterns']:\n",
    "    li = list(i.split(\" \"))\n",
    "    bigger_list.append(li)\n",
    "model= Word2Vec(bigger_list,min_count=1,size=300,workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c067998",
   "metadata": {},
   "outputs": [],
   "source": [
    "bigger_list # check word groupings to see if they make sense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28a93fec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# display the vocabularly from the model\n",
    "vocab = list(model.wv.vocab)\n",
    "vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7beab16f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# view vector for a specific word in the vocab list\n",
    "model.wv[\"goodbye\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11a6bd20",
   "metadata": {},
   "source": [
    "#### Exercise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "241bbd68",
   "metadata": {},
   "source": [
    "create a visualisation of the word embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7d0aafe",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e53605a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# STEP 1 \n",
    "# store all the word vectors (vocab) in a data frame\n",
    "\n",
    "X=model[vocab]\n",
    "vocab_df=pd.DataFrame(X, index = vocab)\n",
    "vocab_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "460d2db4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check vocab length\n",
    "len(vocab_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d05a545",
   "metadata": {},
   "outputs": [],
   "source": [
    "# STEP 2\n",
    "# collapse data using PCA\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "#Computing the correlation matrix\n",
    "X_corr=vocab_df.corr()\n",
    "\n",
    "#Computing eigen values and eigen vectors\n",
    "values,vectors=np.linalg.eig(X_corr)\n",
    "\n",
    "#Sorting the eigen vectors coresponding to eigen values in descending order\n",
    "args = (-values).argsort()\n",
    "values = vectors[args]\n",
    "vectors = vectors[:, args]\n",
    "\n",
    "#Taking first 2 components which explain maximum variance for projecting\n",
    "new_vectors=vectors[:,:2]\n",
    "\n",
    "#Projecting it onto new dimension with 2 axis\n",
    "neww_X=np.dot(X,new_vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1086e7e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# STEP 3 visualise the words\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(13,7))\n",
    "plt.scatter(neww_X[:,0],neww_X[:,1],linewidths=10,color='blue')\n",
    "plt.xlabel(\"PC1\",size=15)\n",
    "plt.ylabel(\"PC2\",size=15)\n",
    "plt.title(\"Word Embedding Space\",size=20)\n",
    "vocab=list(model.wv.vocab)\n",
    "\n",
    "for i, word in enumerate(vocab):\n",
    "  plt.annotate(word,xy=(neww_X[i,0],neww_X[i,1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b38b66a8",
   "metadata": {},
   "source": [
    "Source references with amendments:\n",
    "\n",
    "https://www.guru99.com/word-embedding-word2vec.html\n",
    "\n",
    "https://towardsdatascience.com/visualization-of-word-embedding-vectors-using-gensim-and-pca-8f592a5d3354"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
